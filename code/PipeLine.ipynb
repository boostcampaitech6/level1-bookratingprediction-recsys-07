{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages and install pip\n",
    "### 본 노트북은 다음과 같은 구조일 때 동작합니다\n",
    "```\n",
    "   upper\n",
    "     ├─ /code\n",
    "     |    ├─ /data\n",
    "     |    ├─ /src\n",
    "     |    ├─ /submit\n",
    "     |    ├─ ... \n",
    "     |    ├─ PipeLine.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-23.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#torch 안깔려 있으면, 까셔야 합니다 ..!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /home/minseo/.local/lib/python3.8/site-packages (from -r ../code/requirement.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from -r ../code/requirement.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: nltk in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from -r ../code/requirement.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: transformers in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from -r ../code/requirement.txt (line 4)) (4.36.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/minseo/.local/lib/python3.8/site-packages (from pandas->-r ../code/requirement.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/minseo/.local/lib/python3.8/site-packages (from pandas->-r ../code/requirement.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/minseo/.local/lib/python3.8/site-packages (from pandas->-r ../code/requirement.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from pandas->-r ../code/requirement.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/minseo/.local/lib/python3.8/site-packages (from scikit-learn->-r ../code/requirement.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from scikit-learn->-r ../code/requirement.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from scikit-learn->-r ../code/requirement.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: click in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from nltk->-r ../code/requirement.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from nltk->-r ../code/requirement.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from nltk->-r ../code/requirement.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: filelock in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/minseo/.local/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/minseo/.local/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from transformers->-r ../code/requirement.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ../code/requirement.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ../code/requirement.txt (line 4)) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->-r ../code/requirement.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/minseo/.local/lib/python3.8/site-packages (from requests->transformers->-r ../code/requirement.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from requests->transformers->-r ../code/requirement.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from requests->transformers->-r ../code/requirement.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (from requests->transformers->-r ../code/requirement.txt (line 4)) (2023.11.17)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: easydict in /home/minseo/anaconda3/envs/level1/lib/python3.8/site-packages (1.11)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/minseo/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -r ../code/requirement.txt\n",
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import easydict\n",
    "from src.utils import Logger, Setting, models_load\n",
    "from src.data import context_data_load, context_data_split, context_data_loader\n",
    "from src.data import dl_data_load, dl_data_split, dl_data_loader\n",
    "from src.data import image_data_load, image_data_split, image_data_loader\n",
    "from src.data import text_data_load, text_data_split, text_data_loader\n",
    "from src.train import train, test\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'data_path': './data/',  # Data path 설정\n",
    "    'saved_model_path': './saved_models',  # Saved Model path 설정\n",
    "    'model': \"FM\",  # 학습 및 예측할 모델 선택 (None으로 초기화, 사용 전에 설정 필요)\n",
    "    'data_shuffle': True,  # 데이터 셔플 여부 조정\n",
    "    'test_size': 0.2,  # Train/Valid split 비율 조정\n",
    "    'seed': 42,  # Seed 값 조정\n",
    "    'use_best_model': True,  # 검증 성능이 가장 좋은 모델 사용 여부 설정\n",
    "\n",
    "    # TRAINING OPTION\n",
    "    'batch_size': 1024,  # Batch size 조정\n",
    "    'epochs': 10,  # Epoch 수 조정\n",
    "    'lr': 1e-3,  # Learning Rate 조정\n",
    "    'loss_fn': 'RMSE',  # 손실 함수 변경 (MSE 또는 RMSE)\n",
    "    'optimizer': 'ADAM',  # 최적화 함수 변경 (SGD 또는 ADAM)\n",
    "    'weight_decay': 1e-6,  # Adam optimizer에서 정규화에 사용하는 값 조정\n",
    "\n",
    "    # GPU\n",
    "    'device': 'cuda',  # 학습에 사용할 Device 조정\n",
    "\n",
    "    # FM, FFM, NCF, WDN, DCN Common OPTION\n",
    "    'embed_dim': 16,  # FM, FFM, NCF, WDN, DCN에서 embedding시킬 차원 조정\n",
    "    'dropout': 0.2,  # NCF, WDN, DCN에서 Dropout rate 조정\n",
    "    'mlp_dims': (16, 16),  # NCF, WDN, DCN에서 MLP Network의 차원 조정\n",
    "\n",
    "    # DCN\n",
    "    'num_layers': 3,  # Cross Network의 레이어 수 조정\n",
    "\n",
    "    # CNN_FM\n",
    "    'cnn_embed_dim': 64,  # CNN_FM에서 user와 item에 대한 embedding시킬 차원 조정\n",
    "    'cnn_latent_dim': 12,  # CNN_FM에서 user/item/image에 대한 latent 차원 조정\n",
    "\n",
    "    # DeepCoNN\n",
    "    'vector_create': False,  # DEEP_CONN에서 text vector 생성 여부 조정 (최초 학습에만 True로 설정)\n",
    "    'deepconn_embed_dim': 32,  # DEEP_CONN에서 user와 item에 대한 embedding시킬 차원 조정\n",
    "    'deepconn_latent_dim': 10,  # DEEP_CONN에서 user/item/image에 대한 latent 차원 조정\n",
    "    'conv_1d_out_dim': 50,  # DEEP_CONN에서 1D conv의 출력 크기 조정\n",
    "    'kernel_size': 3,  # DEEP_CONN에서 1D conv의 kernel 크기 조정\n",
    "    'word_dim': 768,  # DEEP_CONN에서 1D conv의 입력 크기 조정\n",
    "    'out_dim': 32  # DEEP_CONN에서 1D conv의 출력 크기 조정\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setting.seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- FM Load Data ---------------\n"
     ]
    }
   ],
   "source": [
    "print(f'--------------- {args.model} Load Data ---------------')\n",
    "if args.model in ('FM', 'FFM'):\n",
    "    data = context_data_load(args)\n",
    "elif args.model in ('NCF', 'WDN', 'DCN'):\n",
    "    data = dl_data_load(args)\n",
    "elif args.model == 'CNN_FM':\n",
    "    data = image_data_load(args)\n",
    "elif args.model == 'DeepCoNN':\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    data = text_data_load(args)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- FM Train/Valid Split ---------------\n"
     ]
    }
   ],
   "source": [
    "######################## Train/Valid Split\n",
    "print(f'--------------- {args.model} Train/Valid Split ---------------')\n",
    "if args.model in ('FM', 'FFM'):\n",
    "    data = context_data_split(args, data)\n",
    "    data = context_data_loader(args, data)\n",
    "\n",
    "elif args.model in ('NCF', 'WDN', 'DCN'):\n",
    "    data = dl_data_split(args, data)\n",
    "    data = dl_data_loader(args, data)\n",
    "\n",
    "elif args.model=='CNN_FM':\n",
    "    data = image_data_split(args, data)\n",
    "    data = image_data_loader(args, data)\n",
    "\n",
    "elif args.model=='DeepCoNN':\n",
    "    data = text_data_split(args, data)\n",
    "    data = text_data_loader(args, data)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Setting for Log\n",
    "setting = Setting()\n",
    "\n",
    "log_path = setting.get_log_path(args)\n",
    "setting.make_dir(log_path)\n",
    "\n",
    "logger = Logger(args, log_path)\n",
    "logger.save_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model (FM 모델을 예시로 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# factorization을 통해 얻은 feature를 embedding 합니다.\n",
    "class FeaturesEmbedding(nn.Module):\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# FM모델 등에서 활용되는 선형 결합 부분을 정의합니다.\n",
    "class FeaturesLinear(nn.Module):\n",
    "    def __init__(self, field_dims: np.ndarray, output_dim: int=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "\n",
    "\n",
    "# feature 사이의 상호작용을 효율적으로 계산합니다.\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, reduce_sum:bool=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "\n",
    "# FM 모델을 구현합니다.\n",
    "class FactorizationMachineModel(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super().__init__()\n",
    "        self.field_dims = data['field_dims']\n",
    "        self.embedding = FeaturesEmbedding(self.field_dims, args.embed_dim)\n",
    "        self.linear = FeaturesLinear(self.field_dims)\n",
    "        self.fm = FactorizationMachine(reduce_sum=True)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear(x) + self.fm(self.embedding(x))\n",
    "        # return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- INIT FM ---------------\n"
     ]
    }
   ],
   "source": [
    "######################## Model\n",
    "print(f'--------------- INIT {args.model} ---------------')\n",
    "model = FactorizationMachineModel(args, data).to(args.device) #이부분수정하면됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RMSEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, x, y):\n",
    "        criterion = MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_loss: 5.015, valid_loss: 2.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:30,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_loss: 2.494, valid_loss: 2.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:06<00:24,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_loss: 2.123, valid_loss: 2.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:09<00:20,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_loss: 1.900, valid_loss: 2.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:11<00:17,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_loss: 1.747, valid_loss: 2.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:17<00:11,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_loss: 1.637, valid_loss: 2.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:20<00:08,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_loss: 1.557, valid_loss: 2.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:22<00:05,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_loss: 1.497, valid_loss: 2.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:25<00:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_loss: 1.453, valid_loss: 2.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_loss: 1.420, valid_loss: 2.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(args, model, data, logger, setting)\n",
    "def train(args, model, dataloader, logger, setting):\n",
    "    minimum_loss = 999999999\n",
    "    if args.loss_fn == 'MSE':\n",
    "        loss_fn = MSELoss()\n",
    "    elif args.loss_fn == 'RMSE':\n",
    "        loss_fn = RMSELoss()\n",
    "    else:\n",
    "        pass\n",
    "    if args.optimizer == 'SGD':\n",
    "        optimizer = SGD(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == 'ADAM':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(args.epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch = 0\n",
    "\n",
    "        for idx, data in enumerate(dataloader['train_dataloader']):\n",
    "            if args.model == 'CNN_FM':\n",
    "                x, y = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "            elif args.model == 'DeepCoNN':\n",
    "                x, y = [data['user_isbn_vector'].to(args.device), data['user_summary_merge_vector'].to(args.device), data['item_summary_vector'].to(args.device)], data['label'].to(args.device)\n",
    "            else:\n",
    "                x, y = data[0].to(args.device), data[1].to(args.device)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y.float(), y_hat)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch +=1\n",
    "        valid_loss = valid(args, model, dataloader, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Train_loss: {total_loss/batch:.3f}, valid_loss: {valid_loss:.3f}')\n",
    "        logger.log(epoch=epoch+1, train_loss=total_loss/batch, valid_loss=valid_loss)\n",
    "        if minimum_loss > valid_loss:\n",
    "            minimum_loss = valid_loss\n",
    "            os.makedirs(args.saved_model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{args.saved_model_path}/{setting.save_time}_{args.model}_model.pt')\n",
    "    logger.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def valid(args, model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "\n",
    "    for idx, data in enumerate(dataloader['valid_dataloader']):\n",
    "        if args.model == 'CNN_FM':\n",
    "            x, y = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        elif args.model == 'DeepCoNN':\n",
    "            x, y = [data['user_isbn_vector'].to(args.device), data['user_summary_merge_vector'].to(args.device), data['item_summary_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        else:\n",
    "            x, y = data[0].to(args.device), data[1].to(args.device)\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y.float(), y_hat)\n",
    "        total_loss += loss.item()\n",
    "        batch +=1\n",
    "    valid_loss = total_loss/batch\n",
    "    return valid_loss\n",
    "\n",
    "\n",
    "def test(args, model, dataloader, setting):\n",
    "    predicts = list()\n",
    "    if args.use_best_model == True:\n",
    "        model.load_state_dict(torch.load(f'./saved_models/{setting.save_time}_{args.model}_model.pt'))\n",
    "    else:\n",
    "        pass\n",
    "    model.eval()\n",
    "\n",
    "    for idx, data in enumerate(dataloader['test_dataloader']):\n",
    "        if args.model == 'CNN_FM':\n",
    "            x, _ = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        elif args.model == 'DeepCoNN':\n",
    "            x, _ = [data['user_isbn_vector'].to(args.device), data['user_summary_merge_vector'].to(args.device), data['item_summary_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        else:\n",
    "            x = data[0].to(args.device)\n",
    "        y_hat = model(x)\n",
    "        predicts.extend(y_hat.tolist())\n",
    "    return predicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- FM PREDICT ---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################## INFERENCE\n",
    "print(f'--------------- {args.model} PREDICT ---------------')\n",
    "predicts = test(args, model, data, setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- SAVE FM PREDICT ---------------\n"
     ]
    }
   ],
   "source": [
    "######################## SAVE PREDICT\n",
    "print(f'--------------- SAVE {args.model} PREDICT ---------------')\n",
    "submission = pd.read_csv(args.data_path + 'sample_submission.csv')\n",
    "if args.model in ('FM', 'FFM', 'NCF', 'WDN', 'DCN', 'CNN_FM', 'DeepCoNN'):\n",
    "    submission['rating'] = predicts\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답안 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = setting.get_submit_filename(args)\n",
    "submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
